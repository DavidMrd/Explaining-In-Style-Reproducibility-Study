{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from resnet_classifier import load_resnet_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True).to(device)\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False).to(device)\n",
    "#model.classifier[1] = nn.Linear(1280, 2).to(device)\n",
    "\n",
    "#model = load_classifier(\"FFHQ-Gender_res64.pth\", 0, 2)\n",
    "#model = load_classifier(\"CelebA-64-nodataaug.pt\", 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noahv/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True).to(device)\n",
    "model.fc = nn.Linear(512, 2).to(device)\n",
    "\n",
    "#model = load_resnet_classifier(\"resnet-18-64px-unfreezel4.pt\", 0, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as vision_F\n",
    "\n",
    "class FFHQ(data.Dataset):\n",
    "    def __init__(self, ffhq_dir, csv_path, image_size=32, transform=None, label=\"gender\"):\n",
    "        \"\"\"\n",
    "        PyTorch DataSet for the FFHQ-Age dataset.\n",
    "        :param root: Root folder that contains a directory for the dataset and the csv with labels in the root directory.\n",
    "        :param label: Label we want to train on, chosen from the csv labels list.\n",
    "        \"\"\"\n",
    "        self.target_class = label\n",
    "\n",
    "        # Store image paths\n",
    "        self.images = [os.path.join(ffhq_dir, file)\n",
    "                       for file in os.listdir(ffhq_dir) if file.endswith('.jpg')]\n",
    "\n",
    "        # Import labels from a CSV file\n",
    "        self.labels = pd.read_csv(csv_path)\n",
    "\n",
    "        def transform_with_resize(tensor_images):\n",
    "            return vision_F.resize(tensor_images, [224, 224])\n",
    "\n",
    "        # Image transformation\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.Lambda(lambda x: transform_with_resize(x)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # Make a lookup dictionary for the labels\n",
    "        # Get column names of dataframe\n",
    "        cols = self.labels.columns.values\n",
    "        label_ids = {col_name: i for i, col_name in enumerate(cols)}\n",
    "        self.class_id = label_ids[self.target_class]\n",
    "\n",
    "        self.one_hot_encoding = {\"male\": 0,\n",
    "                                 \"female\": 1}\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = self.transform(Image.open(self.images[index]))\n",
    "        _label = self.one_hot_encoding[self.labels.iloc[index, self.class_id]]\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class CelebA(data.Dataset):\n",
    "    def __init__(self, celeb_dir, csv_path, image_size=32, transform=None, label=\"male\"):\n",
    "        \"\"\"\n",
    "        PyTorch DataSet for the FFHQ-Age dataset.\n",
    "        :param root: Root folder that contains a directory for the dataset and the csv with labels in the root directory.\n",
    "        :param label: Label we want to train on, chosen from the csv labels list.\n",
    "        \"\"\"\n",
    "        self.target_class = label\n",
    "\n",
    "        # Store image paths\n",
    "        image_path = os.path.join(celeb_dir, \"img_align_celeba\", \"img_align_celeba\")\n",
    "        self.images = [os.path.join(image_path, file)\n",
    "                       for file in os.listdir(image_path) if file.endswith('.jpg')]\n",
    "\n",
    "        # Import labels from a CSV file\n",
    "        self.labels = pd.read_csv(csv_path)\n",
    "\n",
    "        # Image transformation\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # Make a lookup dictionary for the labels\n",
    "        # Get column names of dataframe\n",
    "        cols = self.labels.columns.values\n",
    "        label_ids = {col_name: i for i, col_name in enumerate(cols)}\n",
    "        self.class_id = label_ids[self.target_class]\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = self.transform(Image.open(self.images[index]))\n",
    "        _label = 0 if self.labels.iloc[index, self.class_id] == 1 else 1  # Male will be the first number as with FFHQ upstairs\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_valid_test_dataset(celeba_dir, csv_path, label, image_size=32, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # TODO: Specify different training routines here per class (such as random crop, random horizontal flip, etc.)\n",
    "\n",
    "    dataset = CelebA(celeba_dir, csv_path, image_size=image_size, label=label)\n",
    "    train_length, valid_length, test_length = int(len(dataset) * (1 - valid_ratio - test_ratio)), \\\n",
    "                                              int(len(dataset) * valid_ratio), int(len(dataset) * test_ratio)\n",
    "    # Make sure that the lengths sum to the total length of the dataset\n",
    "    remainder = len(dataset) - train_length - valid_length - test_length\n",
    "    train_length += remainder\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                             [train_length, valid_length, test_length],\n",
    "                                                                             generator=torch.Generator().manual_seed(42)\n",
    "                                                                             )\n",
    "    \"\"\"\n",
    "    train_dataset.set_transform = A.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.RandomBrightnessContrast(p=0.3, brightness_limit=0.25, contrast_limit=0.5),\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.GaussNoise(p=.2),\n",
    "            A.ImageCompression(p=.2, quality_lower=50),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            transforms.Resize(224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "celeb_train, celeb_val, celeb_test = get_train_valid_test_dataset(\"./celeba_data\", \"./celeba_data/list_attr_celeba.csv\", \"Young\", image_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 128\n",
    "cel_train_loader = DataLoader(celeb_train, batch_size=batch_size, pin_memory=True)\n",
    "cel_val_loader = DataLoader(celeb_val, batch_size=batch_size)\n",
    "cel_test_loader = DataLoader(celeb_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.requires_grad_(False)\n",
    "model.fc.requires_grad_(True)\n",
    "\n",
    "def validate_model(model, loader, criterion):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the loss and accuracy.\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # For each batch in the validation set...\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(notebook.tqdm_notebook(loader)):\n",
    "            # Send the batch to the device.\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss += criterion(output, target).item() * len(target)/128\n",
    "\n",
    "            # Get the predictions.\n",
    "            preds = torch.argmax(output, 1)\n",
    "\n",
    "            # Calculate the accuracy.\n",
    "            accuracy += torch.sum(preds == target).item() * len(target)/128\n",
    "\n",
    "    # Calculate the average loss and accuracy.\n",
    "    loss /= len(loader)\n",
    "    accuracy /= len(loader) * batch_size\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, test_model=False, epochs=10):\n",
    "    \"\"\"Trains model\"\"\"\n",
    "\n",
    "    # Put the model in training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch in range(epochs):\n",
    "        # For each batch in the training set...\n",
    "        for batch_idx, (data, target) in enumerate(notebook.tqdm_notebook(train_loader)):\n",
    "            # Send the data and labels to the device.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Zero out the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print the loss.\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(epoch + 1, epochs),\n",
    "                      'Loss: {:.4f}'.format(loss.item()))\n",
    "\n",
    "        # Validate the model.\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Print the validation loss.\n",
    "        print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "        # Print the validation accuracy.\n",
    "        print('Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "\n",
    "    if test_model:\n",
    "        # Test the model.\n",
    "        test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "        # Print the test loss.\n",
    "        print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "        # Print the test accuracy.\n",
    "        print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dd9b4aed9db4471a2ccbb820ae43bfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.9161\n",
      "Epoch: 1/1 Loss: 0.5585\n",
      "Epoch: 1/1 Loss: 0.5584\n",
      "Epoch: 1/1 Loss: 0.5151\n",
      "Epoch: 1/1 Loss: 0.4459\n",
      "Epoch: 1/1 Loss: 0.3432\n",
      "Epoch: 1/1 Loss: 0.4166\n",
      "Epoch: 1/1 Loss: 0.3557\n",
      "Epoch: 1/1 Loss: 0.3853\n",
      "Epoch: 1/1 Loss: 0.3855\n",
      "Epoch: 1/1 Loss: 0.3347\n",
      "Epoch: 1/1 Loss: 0.4234\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1578f75d1f8f4b529382aece0be96c42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4009\n",
      "Validation Accuracy: 0.8213\n"
     ]
    }
   ],
   "source": [
    "# Trained with only classifier unfrozen\n",
    "#model.features.requires_grad_(True)\n",
    "#model.classifier.requires_grad_(True)\n",
    "train_model(model, cel_train_loader, cel_val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16df1c47e5744130a23ed04b78258097"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.4180\n",
      "Epoch: 1/1 Loss: 0.2725\n",
      "Epoch: 1/1 Loss: 0.4427\n",
      "Epoch: 1/1 Loss: 0.3420\n",
      "Epoch: 1/1 Loss: 0.2328\n",
      "Epoch: 1/1 Loss: 0.2085\n",
      "Epoch: 1/1 Loss: 0.3400\n",
      "Epoch: 1/1 Loss: 0.2614\n",
      "Epoch: 1/1 Loss: 0.2582\n",
      "Epoch: 1/1 Loss: 0.3162\n",
      "Epoch: 1/1 Loss: 0.2103\n",
      "Epoch: 1/1 Loss: 0.2890\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "170661b7a2b24b81ae2e63cb54ba2fea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2911\n",
      "Validation Accuracy: 0.8782\n"
     ]
    }
   ],
   "source": [
    "model.layer4.requires_grad_(True)\n",
    "train_model(model, cel_train_loader, cel_val_loader, optimizer, criterion, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af91f055d63e42c199f1e9047b16268e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.2447\n",
      "Epoch: 1/1 Loss: 0.2122\n",
      "Epoch: 1/1 Loss: 0.3601\n",
      "Epoch: 1/1 Loss: 0.2718\n",
      "Epoch: 1/1 Loss: 0.1609\n",
      "Epoch: 1/1 Loss: 0.1964\n",
      "Epoch: 1/1 Loss: 0.3073\n",
      "Epoch: 1/1 Loss: 0.2261\n",
      "Epoch: 1/1 Loss: 0.2123\n",
      "Epoch: 1/1 Loss: 0.2652\n",
      "Epoch: 1/1 Loss: 0.1740\n",
      "Epoch: 1/1 Loss: 0.2032\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca495c6e9789499b9a0449a7ebd074d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2845\n",
      "Validation Accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "model.layer3.requires_grad_(True)\n",
    "train_model(model, cel_train_loader, cel_val_loader, optimizer, criterion, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3d18fe27efa464dac3e4b0910bcbaf7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2846\n",
      "Test Accuracy: 0.8778\n"
     ]
    }
   ],
   "source": [
    "# Test the model.\n",
    "test_loss, test_acc = validate_model(model, cel_test_loader, criterion)\n",
    "\n",
    "# Print the test loss.\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "# Print the test accuracy.\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (15): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (16): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (17): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (18): ConvNormActivation(\n    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU6(inplace=True)\n  )\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze/unfreeze layers after converging with training\n",
    "\n",
    "#model.features[0:15].requires_grad_(False)\n",
    "#model.features[15:].requires_grad_(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f36f2899f7c5408cb51a4648f37704a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.2442\n",
      "Epoch: 1/1 Loss: 0.0951\n",
      "Epoch: 1/1 Loss: 0.0936\n",
      "Epoch: 1/1 Loss: 0.1011\n",
      "Epoch: 1/1 Loss: 0.0920\n",
      "Epoch: 1/1 Loss: 0.2772\n",
      "Epoch: 1/1 Loss: 0.0893\n",
      "Epoch: 1/1 Loss: 0.1182\n",
      "Epoch: 1/1 Loss: 0.0601\n",
      "Epoch: 1/1 Loss: 0.0659\n",
      "Epoch: 1/1 Loss: 0.1026\n",
      "Epoch: 1/1 Loss: 0.1404\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9570e03977fa45f79de55f466708c9f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0667\n",
      "Validation Accuracy: 0.9716\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04fc72fee24344c5a50265a2a3c05050"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0664\n",
      "Test Accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "# Trained with layers until 15 frozen\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (13): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (14): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (15): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (16): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (17): InvertedResidual(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): ConvNormActivation(\n        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (18): ConvNormActivation(\n    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU6(inplace=True)\n  )\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze/unfreeze layers after converging with training\n",
    "\n",
    "model.features[0:13].requires_grad_(False)\n",
    "model.features[13:].requires_grad_(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ef1b22366aa4ee688889b2f46ef981a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.0420\n",
      "Epoch: 1/1 Loss: 0.0411\n",
      "Epoch: 1/1 Loss: 0.0187\n",
      "Epoch: 1/1 Loss: 0.0586\n",
      "Epoch: 1/1 Loss: 0.0439\n",
      "Epoch: 1/1 Loss: 0.1894\n",
      "Epoch: 1/1 Loss: 0.0527\n",
      "Epoch: 1/1 Loss: 0.0850\n",
      "Epoch: 1/1 Loss: 0.0492\n",
      "Epoch: 1/1 Loss: 0.0377\n",
      "Epoch: 1/1 Loss: 0.0611\n",
      "Epoch: 1/1 Loss: 0.0718\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02d32fbc38b44923aacd64f248fb6e64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0562\n",
      "Validation Accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71ad80121ec14d40ae3c5d75be4ac804"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0552\n",
      "Test Accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "# Trained with layers until 13 frozen\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4432 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7872ea193e5b4ecdbbaeedbb19493d70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.3006\n",
      "Epoch: 1/1 Loss: 0.0938\n",
      "Epoch: 1/1 Loss: 0.0645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29568/2936559751.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mgc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequires_grad_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29568/4195540930.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, optimizer, criterion, test_model, epochs)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m             \u001B[1;31m# Update the weights.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 71\u001B[1;33m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     72\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m             \u001B[1;31m# Print the loss.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\optim\\optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 88\u001B[1;33m                     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     89\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\optim\\adam.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    131\u001B[0m                     \u001B[0mstate_steps\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'step'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 133\u001B[1;33m             F.adam(params_with_grad,\n\u001B[0m\u001B[0;32m    134\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    135\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl2021\\lib\\site-packages\\torch\\optim\\_functional.py\u001B[0m in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[1;31m# Decay the first and second moment running average coefficient\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m         \u001B[0mexp_avg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mbeta1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m         \u001B[0mexp_avg_sq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddcmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconj\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mbeta2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m             \u001B[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Only for Resnet, train with all layers unfrozen\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model.requires_grad_(True)\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/950 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3ec3539e544493bba3f2eb8186f1078"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0249\n",
      "Test Accuracy: 0.2408\n"
     ]
    }
   ],
   "source": [
    "# Test the model.\n",
    "test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "# Print the test loss.\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "# Print the test accuracy.\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state dict to file\n",
    "\n",
    "# Trained on 256 works better than 128 for 128\n",
    "torch.save(model.state_dict(), './resnet-18-64px-age-classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_train_loader = DataLoader(train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_valid_test_dataset(ffhq_dir, csv_path, label, image_size=32, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # TODO: Specify different training routines here per class (such as random crop, random horizontal flip, etc.)\n",
    "\n",
    "    dataset = FFHQ(ffhq_dir, csv_path, image_size=image_size, label=label)\n",
    "    train_length, valid_length, test_length = int(len(dataset) * (1 - valid_ratio - test_ratio)), \\\n",
    "                                              int(len(dataset) * valid_ratio), int(len(dataset) * test_ratio)\n",
    "    # Make sure that the lengths sum to the total length of the dataset\n",
    "    remainder = len(dataset) - train_length - valid_length - test_length\n",
    "    train_length += remainder\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                             [train_length, valid_length, test_length],\n",
    "                                                                             generator=torch.Generator().manual_seed(42)\n",
    "                                                                             )\n",
    "    \"\"\"\n",
    "    train_dataset.set_transform = A.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.RandomBrightnessContrast(p=0.3, brightness_limit=0.25, contrast_limit=0.5),\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.GaussNoise(p=.2),\n",
    "            A.ImageCompression(p=.2, quality_lower=50),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            transforms.Resize(224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "train, val, test = get_train_valid_test_dataset(\"./data\", \"./ffhq_aging_labels.csv\", \"gender\", image_size=128)\n",
    "train_loader = DataLoader(train, batch_size=128)\n",
    "val_loader = DataLoader(val, batch_size=128)\n",
    "test_loader = DataLoader(test, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/383 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdae20b6e4914bb5bcef5d0aac48b131"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(train_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/83 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ac681d696944c5ebd5d6517d4475699"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "#y_preds = []\n",
    "#y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(val_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/83 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e81a51af427b4bed8dca9dc83970e904"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "#y_preds = []\n",
    "#y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(test_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.concatenate(y_preds)\n",
    "y_trues = np.concatenate(y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_trues, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[28018,  4152],\n       [ 2494, 35336]], dtype=int64)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Entire dataset:\")\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9050571428571429"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_trues, y_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trying to figure out channel normalization\n",
    "import torch\n",
    "\n",
    "images = torch.rand((5, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "batch_size = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9722, 0.0000],\n        [0.3404, 0.0000]])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = torch.distributions.uniform.Uniform(0, 1).sample([batch_size, 1])\n",
    "torch.cat((torch.ones(batch_size, 1), torch.zeros(batch_size, 1)), dim=1) - torch.cat((samples, torch.zeros((batch_size, 1))), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.7501, 0.2499],\n        [0.5301, 0.4699]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = torch.distributions.uniform.Uniform(0, 1).sample([batch_size, 1])\n",
    "torch.cat((torch.ones(batch_size, 1), torch.zeros(batch_size, 1)), dim=1) - torch.cat((samples, torch.zeros((batch_size, 1))), dim=1) + torch.cat((torch.zeros(batch_size, 1), samples), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfa3962ee0560444ad985082d3a9d1d3cf5b3a106c0ad670dbb0d52cc4dc0741"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}